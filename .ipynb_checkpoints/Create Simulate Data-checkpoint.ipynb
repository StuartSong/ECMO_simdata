{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3d84da-5e90-4477-a253-af59202c1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f89c7b-7d71-4847-8a8d-ec6c57a223f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csn = pd.read_csv(\"/hpc/home/js1207/sparkECMO/Adult ECMO RL/train_data_continuous.csv\")\n",
    "train_csn = train_csn.csn.unique()\n",
    "data = pd.read_csv(\"non_discritized_states.csv\",index_col=0)\n",
    "train_data = data[data['csn'].isin(train_csn)]\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data.drop(columns=['csn'].inplace=True)\n",
    "train_data\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b196a76-44f3-4ab1-909f-a1662987ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csn = pd.read_csv(\"/hpc/home/js1207/sparkECMO/Adult ECMO RL/train_data_continuous.csv\")\n",
    "train_csn = train_csn.csn.unique()\n",
    "data = pd.read_csv(\"non_discritized_states.csv\",index_col=0)\n",
    "train_data = data[data['csn'].isin(train_csn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd225e5-92e8-4002-bd57-2c98f8fdc023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csn</th>\n",
       "      <th>temperature</th>\n",
       "      <th>map_line</th>\n",
       "      <th>map_cuff</th>\n",
       "      <th>pulse</th>\n",
       "      <th>unassisted_resp_rate</th>\n",
       "      <th>end_tidal_co2</th>\n",
       "      <th>o2_flow_rate</th>\n",
       "      <th>base_excess</th>\n",
       "      <th>bicarb_(hco3)</th>\n",
       "      <th>...</th>\n",
       "      <th>procalcitonin</th>\n",
       "      <th>erythrocyte_sedimentation_rate_(esr)</th>\n",
       "      <th>gcs_total_score</th>\n",
       "      <th>best_map</th>\n",
       "      <th>pf_sp</th>\n",
       "      <th>pf_pa</th>\n",
       "      <th>spo2</th>\n",
       "      <th>partial_pressure_of_oxygen_(pao2)</th>\n",
       "      <th>rass_score</th>\n",
       "      <th>CAM_ICU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43640203</td>\n",
       "      <td>37.5000</td>\n",
       "      <td>99.333333</td>\n",
       "      <td>95.4</td>\n",
       "      <td>111.400000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>40.555556</td>\n",
       "      <td>15.092198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.016667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>99.333333</td>\n",
       "      <td>303.125000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>93.425000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43640203</td>\n",
       "      <td>36.8500</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>15.092198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.725000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>94.625000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>94.625000</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>-3.25</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43640203</td>\n",
       "      <td>36.2500</td>\n",
       "      <td>86.333333</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.125000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>35.791667</td>\n",
       "      <td>15.092198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.425000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>86.333333</td>\n",
       "      <td>93.916667</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>93.916667</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43640203</td>\n",
       "      <td>36.5500</td>\n",
       "      <td>78.375000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>117.500000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>30.125000</td>\n",
       "      <td>15.092198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.975000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>7.5</td>\n",
       "      <td>78.375000</td>\n",
       "      <td>93.875000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>93.875000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43640203</td>\n",
       "      <td>37.3500</td>\n",
       "      <td>81.791667</td>\n",
       "      <td>86.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>31.875000</td>\n",
       "      <td>15.092198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>12.0</td>\n",
       "      <td>84.625000</td>\n",
       "      <td>152.252747</td>\n",
       "      <td>154.299451</td>\n",
       "      <td>98.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15187</th>\n",
       "      <td>70840791325</td>\n",
       "      <td>36.5500</td>\n",
       "      <td>75.125000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.124234</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.478012</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>10.0</td>\n",
       "      <td>75.125000</td>\n",
       "      <td>145.555556</td>\n",
       "      <td>234.879583</td>\n",
       "      <td>87.750000</td>\n",
       "      <td>101.879397</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15188</th>\n",
       "      <td>70840791325</td>\n",
       "      <td>36.2500</td>\n",
       "      <td>66.583333</td>\n",
       "      <td>81.0</td>\n",
       "      <td>88.250000</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.124234</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.478012</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.583333</td>\n",
       "      <td>145.555556</td>\n",
       "      <td>234.879583</td>\n",
       "      <td>87.208333</td>\n",
       "      <td>101.879397</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15189</th>\n",
       "      <td>70840791325</td>\n",
       "      <td>36.5500</td>\n",
       "      <td>67.822917</td>\n",
       "      <td>81.0</td>\n",
       "      <td>89.239583</td>\n",
       "      <td>24.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.124234</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.478012</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.822917</td>\n",
       "      <td>145.555556</td>\n",
       "      <td>234.879583</td>\n",
       "      <td>86.756944</td>\n",
       "      <td>101.879397</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15190</th>\n",
       "      <td>70840791325</td>\n",
       "      <td>36.4875</td>\n",
       "      <td>76.416667</td>\n",
       "      <td>81.0</td>\n",
       "      <td>100.166667</td>\n",
       "      <td>18.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.124234</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.478012</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>10.0</td>\n",
       "      <td>76.416667</td>\n",
       "      <td>145.555556</td>\n",
       "      <td>234.879583</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>101.879397</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15191</th>\n",
       "      <td>70840791325</td>\n",
       "      <td>36.3000</td>\n",
       "      <td>51.833333</td>\n",
       "      <td>81.0</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.124234</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.478012</td>\n",
       "      <td>15.945946</td>\n",
       "      <td>10.0</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>145.555556</td>\n",
       "      <td>234.879583</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>101.879397</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11147 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               csn  temperature   map_line  map_cuff       pulse  \\\n",
       "0         43640203      37.5000  99.333333      95.4  111.400000   \n",
       "1         43640203      36.8500  83.250000      95.0   91.750000   \n",
       "2         43640203      36.2500  86.333333      95.0   92.125000   \n",
       "3         43640203      36.5500  78.375000      80.0  117.500000   \n",
       "4         43640203      37.3500  81.791667      86.0  146.000000   \n",
       "...            ...          ...        ...       ...         ...   \n",
       "15187  70840791325      36.5500  75.125000      81.0   86.000000   \n",
       "15188  70840791325      36.2500  66.583333      81.0   88.250000   \n",
       "15189  70840791325      36.5500  67.822917      81.0   89.239583   \n",
       "15190  70840791325      36.4875  76.416667      81.0  100.166667   \n",
       "15191  70840791325      36.3000  51.833333      81.0   30.333333   \n",
       "\n",
       "       unassisted_resp_rate  end_tidal_co2  o2_flow_rate  base_excess  \\\n",
       "0                 17.500000      40.555556     15.092198     0.000000   \n",
       "1                 17.500000      37.500000     15.092198     0.000000   \n",
       "2                 17.500000      35.791667     15.092198     0.000000   \n",
       "3                 17.250000      30.125000     15.092198     0.000000   \n",
       "4                 19.000000      31.875000     15.092198     0.000000   \n",
       "...                     ...            ...           ...          ...   \n",
       "15187             24.000000       0.000000     60.000000     1.124234   \n",
       "15188             23.500000       0.000000     60.000000     1.124234   \n",
       "15189             24.035714       0.000000     60.000000     1.124234   \n",
       "15190             18.833333       0.000000     60.000000     1.124234   \n",
       "15191             19.000000       0.000000     60.000000     1.124234   \n",
       "\n",
       "       bicarb_(hco3)  ...  procalcitonin  \\\n",
       "0          24.016667  ...       0.500000   \n",
       "1          23.725000  ...       0.500000   \n",
       "2          24.425000  ...       0.500000   \n",
       "3          22.975000  ...       0.500000   \n",
       "4          26.300000  ...       0.500000   \n",
       "...              ...  ...            ...   \n",
       "15187      24.000000  ...       6.478012   \n",
       "15188      24.000000  ...       6.478012   \n",
       "15189      24.000000  ...       6.478012   \n",
       "15190      24.000000  ...       6.478012   \n",
       "15191      24.000000  ...       6.478012   \n",
       "\n",
       "       erythrocyte_sedimentation_rate_(esr)  gcs_total_score   best_map  \\\n",
       "0                                 15.945946              3.0  99.333333   \n",
       "1                                 15.945946              3.0  83.250000   \n",
       "2                                 15.945946              3.0  86.333333   \n",
       "3                                 15.945946              7.5  78.375000   \n",
       "4                                 15.945946             12.0  84.625000   \n",
       "...                                     ...              ...        ...   \n",
       "15187                             15.945946             10.0  75.125000   \n",
       "15188                             15.945946             10.0  66.583333   \n",
       "15189                             15.945946             10.0  67.822917   \n",
       "15190                             15.945946             10.0  76.416667   \n",
       "15191                             15.945946             10.0  61.500000   \n",
       "\n",
       "            pf_sp       pf_pa       spo2  partial_pressure_of_oxygen_(pao2)  \\\n",
       "0      303.125000   67.000000  93.425000                          67.000000   \n",
       "1       94.625000   69.500000  94.625000                          69.500000   \n",
       "2       93.916667   74.000000  93.916667                          74.000000   \n",
       "3       93.875000   70.500000  93.875000                          70.500000   \n",
       "4      152.252747  154.299451  98.750000                         100.000000   \n",
       "...           ...         ...        ...                                ...   \n",
       "15187  145.555556  234.879583  87.750000                         101.879397   \n",
       "15188  145.555556  234.879583  87.208333                         101.879397   \n",
       "15189  145.555556  234.879583  86.756944                         101.879397   \n",
       "15190  145.555556  234.879583  80.500000                         101.879397   \n",
       "15191  145.555556  234.879583  61.333333                         101.879397   \n",
       "\n",
       "       rass_score  CAM_ICU  \n",
       "0           -4.00     1.00  \n",
       "1           -3.25     1.00  \n",
       "2           -3.00     1.00  \n",
       "3           -2.25     0.25  \n",
       "4            0.75     0.00  \n",
       "...           ...      ...  \n",
       "15187       -2.00     0.00  \n",
       "15188       -2.00     0.00  \n",
       "15189       -2.00     0.00  \n",
       "15190       -2.00     0.00  \n",
       "15191       -2.00     0.00  \n",
       "\n",
       "[11147 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b738550b-4a8d-40aa-a9e0-ad0ef0e0e92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 0.932\n",
      "Epoch 2/100, Average Loss: 0.741\n",
      "Epoch 3/100, Average Loss: 0.637\n",
      "Epoch 4/100, Average Loss: 0.574\n",
      "Epoch 5/100, Average Loss: 0.532\n",
      "Epoch 6/100, Average Loss: 0.504\n",
      "Epoch 7/100, Average Loss: 0.482\n",
      "Epoch 8/100, Average Loss: 0.468\n",
      "Epoch 9/100, Average Loss: 0.454\n",
      "Epoch 10/100, Average Loss: 0.445\n",
      "Epoch 11/100, Average Loss: 0.436\n",
      "Epoch 12/100, Average Loss: 0.427\n",
      "Epoch 13/100, Average Loss: 0.420\n",
      "Epoch 14/100, Average Loss: 0.414\n",
      "Epoch 15/100, Average Loss: 0.409\n",
      "Epoch 16/100, Average Loss: 0.404\n",
      "Epoch 17/100, Average Loss: 0.400\n",
      "Epoch 18/100, Average Loss: 0.395\n",
      "Epoch 19/100, Average Loss: 0.393\n",
      "Epoch 20/100, Average Loss: 0.389\n",
      "Epoch 21/100, Average Loss: 0.386\n",
      "Epoch 22/100, Average Loss: 0.383\n",
      "Epoch 23/100, Average Loss: 0.381\n",
      "Epoch 24/100, Average Loss: 0.378\n",
      "Epoch 25/100, Average Loss: 0.377\n",
      "Epoch 26/100, Average Loss: 0.374\n",
      "Epoch 27/100, Average Loss: 0.373\n",
      "Epoch 28/100, Average Loss: 0.372\n",
      "Epoch 29/100, Average Loss: 0.368\n",
      "Epoch 30/100, Average Loss: 0.369\n",
      "Epoch 31/100, Average Loss: 0.367\n",
      "Epoch 32/100, Average Loss: 0.365\n",
      "Epoch 33/100, Average Loss: 0.365\n",
      "Epoch 34/100, Average Loss: 0.362\n",
      "Epoch 35/100, Average Loss: 0.362\n",
      "Epoch 36/100, Average Loss: 0.361\n",
      "Epoch 37/100, Average Loss: 0.359\n",
      "Epoch 38/100, Average Loss: 0.358\n",
      "Epoch 39/100, Average Loss: 0.357\n",
      "Epoch 40/100, Average Loss: 0.356\n",
      "Epoch 41/100, Average Loss: 0.355\n",
      "Epoch 42/100, Average Loss: 0.354\n",
      "Epoch 43/100, Average Loss: 0.353\n",
      "Epoch 44/100, Average Loss: 0.352\n",
      "Epoch 45/100, Average Loss: 0.351\n",
      "Epoch 46/100, Average Loss: 0.350\n",
      "Epoch 47/100, Average Loss: 0.348\n",
      "Epoch 48/100, Average Loss: 0.348\n",
      "Epoch 49/100, Average Loss: 0.347\n",
      "Epoch 50/100, Average Loss: 0.346\n",
      "Epoch 51/100, Average Loss: 0.345\n",
      "Epoch 52/100, Average Loss: 0.345\n",
      "Epoch 53/100, Average Loss: 0.344\n",
      "Epoch 54/100, Average Loss: 0.344\n",
      "Epoch 55/100, Average Loss: 0.341\n",
      "Epoch 56/100, Average Loss: 0.341\n",
      "Epoch 57/100, Average Loss: 0.341\n",
      "Epoch 58/100, Average Loss: 0.340\n",
      "Epoch 59/100, Average Loss: 0.339\n",
      "Epoch 60/100, Average Loss: 0.338\n",
      "Epoch 61/100, Average Loss: 0.338\n",
      "Epoch 62/100, Average Loss: 0.337\n",
      "Epoch 63/100, Average Loss: 0.336\n",
      "Epoch 64/100, Average Loss: 0.337\n",
      "Epoch 65/100, Average Loss: 0.337\n",
      "Epoch 66/100, Average Loss: 0.336\n",
      "Epoch 67/100, Average Loss: 0.334\n",
      "Epoch 68/100, Average Loss: 0.333\n",
      "Epoch 69/100, Average Loss: 0.334\n",
      "Epoch 70/100, Average Loss: 0.334\n",
      "Epoch 71/100, Average Loss: 0.333\n",
      "Epoch 72/100, Average Loss: 0.332\n",
      "Epoch 73/100, Average Loss: 0.332\n",
      "Epoch 74/100, Average Loss: 0.332\n",
      "Epoch 75/100, Average Loss: 0.332\n",
      "Epoch 76/100, Average Loss: 0.331\n",
      "Epoch 77/100, Average Loss: 0.331\n",
      "Epoch 78/100, Average Loss: 0.330\n",
      "Epoch 79/100, Average Loss: 0.330\n",
      "Epoch 80/100, Average Loss: 0.329\n",
      "Epoch 81/100, Average Loss: 0.330\n",
      "Epoch 82/100, Average Loss: 0.329\n",
      "Epoch 83/100, Average Loss: 0.329\n",
      "Epoch 84/100, Average Loss: 0.328\n",
      "Epoch 85/100, Average Loss: 0.328\n",
      "Epoch 86/100, Average Loss: 0.328\n",
      "Epoch 87/100, Average Loss: 0.327\n",
      "Epoch 88/100, Average Loss: 0.327\n",
      "Epoch 89/100, Average Loss: 0.327\n",
      "Epoch 90/100, Average Loss: 0.326\n",
      "Epoch 91/100, Average Loss: 0.326\n",
      "Epoch 92/100, Average Loss: 0.325\n",
      "Epoch 93/100, Average Loss: 0.326\n",
      "Epoch 94/100, Average Loss: 0.325\n",
      "Epoch 95/100, Average Loss: 0.325\n",
      "Epoch 96/100, Average Loss: 0.324\n",
      "Epoch 97/100, Average Loss: 0.325\n",
      "Epoch 98/100, Average Loss: 0.324\n",
      "Epoch 99/100, Average Loss: 0.324\n",
      "Epoch 100/100, Average Loss: 0.325\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# We'll assume 'train_data' is already defined as a pandas dataframe with shape (11147, 43)\n",
    "\n",
    "class MLPVAE(nn.Module):\n",
    "    def __init__(self, input_dim=43, hidden_dim=64, latent_dim=16):\n",
    "        super(MLPVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = ((recon_x - x) ** 2).sum(dim=0)  # Compute loss per column\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss, kld\n",
    "\n",
    "# Convert your dataframe to a torch tensor\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "\n",
    "# Create a simple dataset and dataloader\n",
    "class TabularDataset(data.Dataset):\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor[idx]\n",
    "    def __len__(self):\n",
    "        return self.tensor.shape[0]\n",
    "\n",
    "dataset = TabularDataset(train_tensor)\n",
    "dataloader = data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer\n",
    "model = MLPVAE(input_dim=43, hidden_dim=64, latent_dim=16)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_recon_loss = torch.zeros(train_tensor.shape[1])\n",
    "    total_kld = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = model(batch)\n",
    "        recon_loss, kld = vae_loss(recon, batch, mu, logvar)\n",
    "        loss = recon_loss.sum() + kld  # Total loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.detach()\n",
    "        total_kld += kld.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_recon_loss = total_recon_loss / (num_batches * dataloader.batch_size)\n",
    "    avg_kld = total_kld / (num_batches * dataloader.batch_size)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_recon_loss.numpy().mean():.3f}\")\n",
    "    # print(f\"Avg per-column reconstruction loss: {}\")\n",
    "\n",
    "torch.save(model, \"mlp_vae.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c19ffeb-2e26-4d56-b39f-1d8396f2ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"mlp_vae.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ba41906-4cd5-4772-a9ce-cabb0f7ffdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified synthetic output: [-1.1699022   0.42113316  0.46528575  0.38823992  0.3147081  -0.24146605\n",
      "  1.0364234   0.01297785  0.03269807 -0.16800848 -0.03724345  0.23910508\n",
      " -0.01513278  1.3327388  -0.64124554  1.1138294   0.54673445 -1.0044123\n",
      " -0.08894157 -0.16174182 -0.3544161  -0.11003835  1.2203125   0.14217015\n",
      " -0.36982304 -0.2851519   1.8047109  -0.02481666  0.06410962 -0.6459687\n",
      "  0.07594274  0.05963591 -0.62476987 -0.12943527  0.35917825 -0.95582557\n",
      "  0.5780245  -0.17491864 -0.79985195 -0.08835298 -0.7651638  -0.874994\n",
      "  1.542209  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2198607/2336691718.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"mlp_vae.pth\")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"mlp_vae.pth\")\n",
    "model.eval()  # Set to evaluation mode if not training\n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_synthetic_data(model, sample, column_idx, new_value):\n",
    "    \"\"\"\n",
    "    Modifies a specific column in the input and generates a synthetic output.\n",
    "    \n",
    "    :param model: Trained MLPVAE model\n",
    "    :param sample: A single input sample (1D tensor)\n",
    "    :param column_idx: Index of the column to modify\n",
    "    :param new_value: New value to assign to the column\n",
    "    :return: Generated output with modified column\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to batch format (1 sample)\n",
    "    sample = sample.clone().unsqueeze(0)  # Shape (1, input_dim)\n",
    "    \n",
    "    # Encode to latent space\n",
    "    with torch.no_grad():\n",
    "        mu, logvar = model.encode(sample)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "\n",
    "    # Modify the column in latent space (alternative: modify directly in input)\n",
    "    modified_sample = sample.clone()\n",
    "    modified_sample[0, column_idx] = new_value  # Change the specified column\n",
    "\n",
    "    # Re-encode after modification\n",
    "    with torch.no_grad():\n",
    "        new_mu, new_logvar = model.encode(modified_sample)\n",
    "        new_z = model.reparameterize(new_mu, new_logvar)\n",
    "\n",
    "    # Decode back to see changes\n",
    "    generated_output = model.decode(new_z)\n",
    "\n",
    "    return generated_output.squeeze().detach().numpy()  # Convert back to NumPy for easier analysis\n",
    "\n",
    "# Example usage:\n",
    "sample_idx = 0  # Pick any row from your dataset\n",
    "sample_data = train_tensor[sample_idx]  # Original sample\n",
    "\n",
    "column_to_change = 5  # Example: Modify column 5\n",
    "new_value = 2.0  # New value to assign\n",
    "\n",
    "synthetic_output = generate_synthetic_data(model, sample_data, column_to_change, new_value)\n",
    "\n",
    "print(\"Modified synthetic output:\", synthetic_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57ae03d8-6be4-4c93-b6c3-15666ff28bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   43640203,  2178911314,  2207761225,  4510511039,  6648317299,\n",
       "        7270121193, 10017307254, 10057670084, 10243791111, 11197349249,\n",
       "       11367061032, 11598700132, 12128379018, 12156649330, 14719067105,\n",
       "       15668590152, 15738357227, 16251981187, 17249721021, 19073406214,\n",
       "       19085650202, 19330760302, 19657017003, 31978091155, 32313439276,\n",
       "       36950570009, 38450329331, 39299217144, 42008659061, 42989860004,\n",
       "       43323090183, 43917519266, 43946031211, 44991746336, 45457761150,\n",
       "       46056470115, 46429729018, 48783979065, 48791437223, 48994390085,\n",
       "       51299946263, 52278286092, 52461796114, 53029789123, 53045406183,\n",
       "       53096266192, 53243526208, 53393046226, 53467166235, 53476486236,\n",
       "       54565637009, 54705307026, 55395459196, 55403120241, 55511607117,\n",
       "       55953247177, 56063101211, 56258940008, 56831407291, 57284130211,\n",
       "       57642960169, 58928481104, 60780209038, 60845329119, 60879489278,\n",
       "       60956019056, 61200959078, 61260430112, 61391379093, 62369850199,\n",
       "       62405229244, 63083139237, 63121461075, 63152719298, 63433549223,\n",
       "       63512809230, 63869689272, 64309929313, 64723420073, 64778739353,\n",
       "       64843720260, 64908640003, 64939920143, 65075060016, 65098750021,\n",
       "       65157230024, 65189640027, 65209900029, 65264380035, 65390550048,\n",
       "       65682400100, 65706900107, 65816090128, 65968410147, 66144110163,\n",
       "       66303050184, 66322050188, 66385050238, 66614490225, 66713900239,\n",
       "       66713901041, 66724300239, 66795550261, 66803750251, 66926880266,\n",
       "       67152710303, 67307200314, 67448150349, 67546400366, 67620281055,\n",
       "       67682541003, 67763261127, 67779421014, 67786001015, 67796811017,\n",
       "       67843131173, 67872761027, 67958411160, 68121041056, 68177151063,\n",
       "       68192231065, 68315071120, 68321591079, 68349741082, 68463091091,\n",
       "       68780921116, 68797341117, 68845081130, 68862911124, 68909051127,\n",
       "       68989721134, 69241991159, 69356011171, 69372091173, 69487791185,\n",
       "       69567811194, 69626971200, 69649561202, 69746211211, 69900791287,\n",
       "       70201931257, 70225691258, 70293951265, 70341401291, 70454151283,\n",
       "       70666401306, 70840791325])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e43ad66d-d0e8-4730-ac36-38cb2af400c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2854671802.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    sparkECMO/Adult ECMO RL/train_data_continuous.csn\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sparkECMO/Adult ECMO RL/train_data_continuous.csn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8320004-f259-4713-be10-4ae8356d2662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
